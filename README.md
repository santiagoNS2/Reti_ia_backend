# RetoÂ Backend â€“ OCRÂ + IA (FastAPIÂ + Mistral)

Este repositorio resuelve el desafÃ­o tÃ©cnico de construir una API capaz de:

1. **Procesar documentos PDF e imÃ¡genes** mediante OCR (Tesseract).
2. **Generar un resumen** y **extraer entidades clave** usando un modelo LLM local (Mistral vÃ­aÂ Ollama).
3. Mantener un **historial** de documentos procesados y exponerlo vÃ­a API.
4. Servir un **frontend mÃ­nimo** para subir archivos y visualizar resultados.

---

## ğŸŒ Demo rÃ¡pida

| Recurso               | URL por defecto                                          | QuÃ© verÃ¡s                                                     |
| --------------------- | -------------------------------------------------------- | ------------------------------------------------------------- |
| Frontend              | [http://localhost:8000](http://localhost:8000)           | Formulario para subir PDF/imagen y la tabla *Historial*.      |
| DocumentaciÃ³n Swagger | [http://localhost:8000/docs](http://localhost:8000/docs) | Prueba los endpoints `POST /api/upload` y `GET /api/history`. |

---

## ğŸ“‚ Estructura del proyecto

```
Reto_backend_IA/
â”œâ”€â”€ app/                # CÃ³digo FastAPI
â”‚   â”œâ”€â”€ api/            # Rutas (endpoints)
â”‚   â”œâ”€â”€ services/       # OCRÂ y LLM helper functions
â”‚   â””â”€â”€ main.py         # Instancia FastAPI + montaje frontend
â”œâ”€â”€ frontend/           # index.html, assets
â”œâ”€â”€ archivos/           # PDFs, imÃ¡genes subidas y history.json
â”œâ”€â”€ Dockerfile          # Imagen de la API + Tesseract
â”œâ”€â”€ docker-compose.yml  # Levanta API + Ollama
â””â”€â”€ requirements.txt    # Dependencias Python
```

> **Nota**: el OCR guarda los PDFs y las imÃ¡genes tal cual se reciben en `archivos/uploads/`; el archivo `archivos/history.json` almacena el texto extraÃ­do, el resumen y las entidades de cada documento.

---

## ğŸš€ Puesta en marcha

### 1. Clonar el proyecto

```bash
git clone https://github.com/santiagoNS2/Reto_Backend-IA_savant.git
cd Reto_Backend-IA_savant
```

### 2. EjecuciÃ³n mÃ¡s sencilla: **DockerÂ Compose**

```bash
docker compose up --build
```

Esto construye la imagen de la API y arranca dos contenedores:

| Servicio | Imagen          | Puerto   | DescripciÃ³n                                  |
| -------- | --------------- | -------- | -------------------------------------------- |
| `web`    | reto-backend-ia | **8000** | FastAPI + Frontend                           |
| `ollama` | ollama/ollama   | 11434    | Servidor Ollama cargando el modelo *Mistral* |

> La primera vez, `ollama` ejecuta `ollama pull mistral`, puede tardar unos minutos.

### 3. EjecuciÃ³n manual (sin Compose)

1. **Ollama**

   ```bash
   # TerminalÂ A
   ollama serve &
   ollama pull mistral   # descarga el modelo (~4Â GB)
   ```
2. **API**

   ```bash
   # TerminalÂ B
   docker build -t reto-backend-ia .
   docker run -p 8000:8000 \
     -e OLLAMA_URL=http://host.docker.internal:11434 \
     reto-backend-ia
   ```

(Usa `host.docker.internal` para que el contenedor vea el host donde corre Ollama.)

---

## ğŸ–¥ï¸ Uso del Frontend

1. Abre [http://localhost:8000](http://localhost:8000).
2. Haz clic en **â€œElegir archivoâ€** y selecciona un PDF o imagen.
3. Pulsa **â€œSubir y Procesarâ€**.
4. La secciÃ³n **Resultado Reciente** muestra el resumen y las entidades del archivo.
5. MÃ¡s abajo, **Historial** lista todos los documentos procesados (nombre, fecha, resumen y entidades).

---

## ğŸ”§ Cambiar de modelo LLM

Si dispones de mÃ¡s recursos y quieres usar otro modelo aceleradoÂ GPUÂ o mÃ¡s grande:

1. Descarga el modelo con `ollama pull <modelo>` (ej. `llama3`).
2. Edita \`\` â†’ lÃ­neaÂ 5:

   ```python
   MODEL_NAME = "llama3"  # antes: "mistral"
   ```
3. Reconstruye la imagen y vuelve a levantar los contenedores.

---

## âš™ï¸ Requisitos para ejecuciÃ³n local (sin Docker)

| Tipo          | VersiÃ³n mÃ­nima                                           |
| ------------- | -------------------------------------------------------- |
| Python        | Â 3.11                                                    |
| TesseractÂ OCR | Â 5.x + paquetes `tesseract-ocr-spa`, `tesseract-ocr-eng` |
| Poppler       | Para `pdf2image`                                         |
| Ollama        | â‰¥Â 0.1.34                                                 |

Instala dependencias Python:

```bash
pip install -r requirements.txt
```

Arranca Ollama y luego `uvicorn app.main:app --reload` para desarrollo.

---

## ğŸ”’ Licencia

MIT Â© 2025 SantiagoÂ NS2 â€“ Este proyecto se publica con fines acadÃ©micos y puede usarse libremente citando la autorÃ­a.
