# RetoÂ Backend â€“ OCRÂ + IA (FastAPIÂ + Mistral "ollama")

**Autor: Santiago Naranjo Sanchez**

Este repositorio resuelve el desafÃ­o tÃ©cnico de construir una API capaz de:

1. **Procesar documentos PDF e imÃ¡genes** mediante OCR (Tesseract).
2. **Generar un resumen** y **extraer entidades clave** usando un modelo LLM local (Mistral vÃ­aÂ Ollama).
3. Mantener un **historial** de documentos procesados y exponerlo vÃ­a API.
4. Servir un **frontend mÃ­nimo** para subir archivos y visualizar resultados.

---

## ğŸŒ Demo rÃ¡pida

| Recurso               | URL por defecto                                          | QuÃ© verÃ¡s                                                     |
| --------------------- | -------------------------------------------------------- | ------------------------------------------------------------- |
| Frontend              | [http://localhost:8000](http://localhost:8000)           | Formulario para subir PDF/imagen y la tabla *Historial*.      |
| DocumentaciÃ³n Swagger | [http://localhost:8000/docs](http://localhost:8000/docs) | Prueba los endpoints `POST /api/upload` y `GET /api/history`. |

---

## ğŸ“‚ Estructura del proyecto

```
Reto_backend_IA/
â”œâ”€â”€ app/                # CÃ³digo FastAPI
â”‚   â”œâ”€â”€ api/            # Rutas (endpoints)
|   |â”€â”€ frontend        # index.html, assets
â”‚   â”œâ”€â”€ services/       # OCRÂ y LLM helper functions
â”‚   â””â”€â”€ main.py         # Instancia FastAPI + montaje frontend          
â”œâ”€â”€ archivos/           # PDFs, imÃ¡genes subidas y history.json
â”œâ”€â”€ Dockerfile          # Imagen de la API + Tesseract
â”œâ”€â”€ docker-compose.yml  # Levanta API + Ollama
â””â”€â”€ requirements.txt    # Dependencias Python
```

> **Nota**: el OCR guarda los PDFs y las imÃ¡genes tal cual se reciben en `archivos/uploads/`; el archivo `archivos/history.json` almacena el texto extraÃ­do, el resumen y las entidades de cada documento.

---

## ğŸš€ Puesta en marcha

### 1. Clonar el proyecto

```bash
git clone https://github.com/santiagoNS2/Reto_Backend-IA_savant.git
cd Reto_Backend-IA_savant
```

### 2. EjecuciÃ³n mÃ¡s sencilla: **DockerÂ Compose**

```bash
docker compose up --build
```

Esto construye la imagen de la API y arranca dos contenedores:

| Servicio | Imagen          | Puerto   | DescripciÃ³n                                  |
| -------- | --------------- | -------- | -------------------------------------------- |
| `web`    | reto-backend-ia | **8000** | FastAPI + Frontend                           |
| `ollama` | ollama/ollama   | 11434    | Servidor Ollama cargando el modelo *Mistral* |

> La primera vez, `ollama` ejecuta `ollama pull mistral`, puede tardar unos minutos.

### 3. EjecuciÃ³n manual (sin Compose)

1. **Ollama**

   ```bash
   # TerminalÂ A
   ollama serve &
   ollama pull mistral   # descarga el modelo (~4Â GB)
   ```
2. **API**

   ```bash
   # TerminalÂ B
   docker build -t reto-backend-ia .
   docker run -p 8000:8000 \
     -e OLLAMA_URL=http://host.docker.internal:11434 \
     reto-backend-ia
   ```

(Usa `host.docker.internal` para que el contenedor vea el host donde corre Ollama.)

---

## ğŸ–¥ï¸ Uso del Frontend

1. Abre [http://localhost:8000](http://localhost:8000).
2. Haz clic en **â€œElegir archivoâ€** y selecciona un PDF o imagen.
3. Pulsa **â€œSubir y Procesarâ€**.
4. La secciÃ³n **Resultado Reciente** muestra el resumen y las entidades del archivo.
5. MÃ¡s abajo, **Historial** lista todos los documentos procesados (nombre, fecha, resumen y entidades).

---

## ğŸ”§ Cambiar de modelo LLM

Si dispones de mÃ¡s recursos y quieres usar otro modelo aceleradoÂ GPUÂ o mÃ¡s grande:

1. Descarga el modelo con `ollama pull <modelo>` (ej. `llama3`).
2. Edita \`\` â†’ lÃ­neaÂ 5:

   ```python
   MODEL_NAME = "llama3"  # antes: "mistral"
   ```
3. Reconstruye la imagen y vuelve a levantar los contenedores.

---

## âš™ï¸ Requisitos para ejecuciÃ³n local (sin Docker)

| Tipo          | VersiÃ³n mÃ­nima                                           |
| ------------- | -------------------------------------------------------- |
| Python        | Â 3.11                                                    |
| TesseractÂ OCR | Â 5.x + paquetes `tesseract-ocr-spa`, `tesseract-ocr-eng` |
| Poppler       | Para `pdf2image`                                         |
| Ollama        | â‰¥Â 0.1.34                                                 |

Instala dependencias Python:

```bash
pip install -r requirements.txt
```

Arranca Ollama y luego `uvicorn app.main:app --reload` para desarrollo.

---

## ğŸ”® Â¿QuÃ© usarÃ­a en producciÃ³n y por quÃ©?

> **Pregunta del reto:** â€œ*Documenta claramente quÃ© usarÃ­as en producciÃ³n (ej. OpenAI, Gemini, etc.)*â€.

| OpciÃ³n                                       | Ventajas clave                                                                                                                                                          | Desventajas                                                                                                       |
| -------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------- |
| **OpenAIÂ GPTâ€‘4o**                            | PrecisiÃ³n SOTA en comprensiÃ³n+razonamiento; funciones de "tool calling"; latencia global baja (regiones mÃºltiples); ecosistema maduro (embeddings, moderation, vision). | CÃ³digo cerrado; exige enviar datos fuera de la org; precio superior a modelos openâ€‘source si la escala es grande. |
| **GoogleÂ GeminiÂ 1.5Â Pro**                    | Context window masivo (1Â M tokens); buen *multimodal*; integraciÃ³n nativa con GoogleÂ Cloud.                                                                             | AÃºn en beta en algunos paÃ­ses; ecosistema de plugins mÃ¡s pequeÃ±o.                                                 |
| **DeepSeekâ€‘V2Â 67B**                          | Modelo openâ€‘source mixto (inglÃ©sâ€‘chino) con excelente rendimiento en benchmarks; licencia ApacheÂ 2; menor tamaÃ±o que Llamaâ€‘3Â 70B â‡’ +rÃ¡pido y menos RAM.                 | Comunidad aÃºn pequeÃ±a; soporte multilingÃ¼e (ES) mÃ¡s limitado; requiere operar infraestructura GPU propia.         |
| **Openâ€‘source (Llamaâ€‘3Â 70B, MixtralÂ 8x22B)** | Datos onâ€‘prem; personalizaciÃ³n vÃ­a LoRA; coste variableÂ â‰ˆÂ 0 una vez desplegado.                                                                                         | Requiere operar infraestructura GPU y MLOps; menor desempeÃ±o en tareas complejas que GPTâ€‘4o.                      |

**Si el precio no es limitante**, elegirÃ­a **GPTâ€‘4o** para la versiÃ³n de producciÃ³n porque entrega:

1. **MÃ¡xima precisiÃ³n** en generaciÃ³n de resÃºmenes y NER sin finos ajustes.
2. **Latencia global** con redundancia (AzureÂ OpenAI +Â OpenAI).
3. **Mantenimiento casi nulo** (no hay que actualizar pesos ni escalar clustersÂ GPU).

Para escenarios **onâ€‘prem** o con datos altamente sensibles, migrarÃ­a a **Llamaâ€‘3Â 70B** o **DeepSeekâ€‘V2 67B** desplegados en Kubernetes con vLLM.

\--------|----------------|-------------|
\| **OpenAIÂ GPTâ€‘4o** | PrecisiÃ³n SOTA en comprensiÃ³n+razonamiento; funciones de "tool calling"; latencia global baja (regiones mÃºltiples); ecosistema maduro (embeddings, moderation, vision). | CÃ³digo cerrado; exige enviar datos fuera de la org; precio superior a modelos openâ€‘source si la escala es grande. |
\| **GoogleÂ GeminiÂ 1.5Â Pro** | Context window masivo (1Â M tokens); buen *multimodal*; integraciÃ³n nativa con GoogleÂ Cloud. | AÃºn en beta en algunos paÃ­ses; ecosistema de plugins mÃ¡s pequeÃ±o. |
\| **Openâ€‘source (Llamaâ€‘3 70B, MixtralÂ 8x22B)** | Datos onâ€‘prem; personalizaciÃ³n vÃ­a LoRA; coste variableÂ â‰ˆÂ 0 una vez desplegado. | Requiere operar infraestructura GPU y MLOps; menor desempeÃ±o en tareas complejas que GPTâ€‘4o. |

**Si el precio no es limitante**, elegirÃ­a **GPTâ€‘4o** para la versiÃ³n de producciÃ³n porque entrega:

1. **MÃ¡xima precisiÃ³n** en generaciÃ³n de resÃºmenes y NER sin finos ajustes.
2. **Latencia global** con redundancia (AzureÂ OpenAI +Â OpenAI).
3. **Mantenimiento casi nulo** (no hay que actualizar pesos ni escalar clusters GPU).

Para escenarios **onâ€‘prem** o con datos altamente sensibles, migrarÃ­a a **Llamaâ€‘3Â 70B** desplegado enÂ Kubernetes con vLLM.

---




